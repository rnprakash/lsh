\documentclass[a4paper]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{parskip,amsmath,mathtools,amssymb,amsfonts,mathrsfs}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{url,titling}
\usepackage{fancyhdr,hyperref}
\usepackage{booktabs,color,tabularx}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\innerproduct}[2]{\langle{}#1,#2\rangle{}}
\newtheoremstyle{def}
{8pt}
{5pt}
{}
{}
{\bfseries}
{:}
{.5em}
{}

\newtheoremstyle{thm}
{8pt}
{5pt}
{\itshape}
{}
{\bfseries}
{:}
{.5em}
{}

\theoremstyle{def}
\newtheorem{definition}{Definition}
\newtheorem{assumption}[definition]{Assumption}
\theoremstyle{thm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{claim}[proposition]{Claim}
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}{Corollary}[proposition]
%\newenvironment{proof}[1][]{\textbf{Proof:} }{\hfill$\square$}

%\setlength{\textfloatsep}{6pt}

% Math symbols
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\newcommand{\A}[0]{\mathcal{A}}
\newcommand{\e}[1]{\text{e}^{#1}}
\newcommand{\E}[1]{\mathbf{E}\left[#1\right]}
\newcommand{\p}[0]{\mathbf{p}}
\newcommand{\q}[0]{\mathbf{q}}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\LSH}[0]{\mathcal{H}}
\newcommand{\re}[1]{\frac{1}{#1}}
\newcommand{\set}[1]{\lbrace\ 0,1 \rbrace^{#1}}
\newcommand{\X}[0]{\mathcal{X}}
\newcommand{\Y}[0]{\mathcal{Y}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\aand}[0]{\text{ and }}
\newcommand{\iif}[0]{\text{ if }}
\newcommand{\ow}[0]{\text{ otherwise}}

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO\@: {#1}}}

\setlength{\droptitle}{-8em}
\title{Ranked Leakage-Sensitive Hashing}
\author{Rohith Prakash}
\date{}

\begin{document}
\maketitle{}

\begin{abstract}
    In this paper, we study the problem of real-time anomaly detection of intelligent malware samples disguised within benign applications. 
    We demonstrate that commonly used Dynamic Time Warping (DTW) distance is not suitable on time series of system resource traces when malware samples dynamically adapt their behavior to evade detection.
    To deal with malware samples in real-time which attempt to hide within benign behavior, we propose a new LSH-based scheme that has low hardware and complexity overhead, iteratively hashes time series based on \textit{behavioral patterns}, triangulates hashing buckets to better categorize behavior, and is able to categorize new, previously unobserved behavior.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Time series are used almost ubiquitously to represent time-based measurements in various fields.
However, it is a well known problem that when these times series represent behavior-related observations of complex systems, unintended information about the system may be leaked through this channel~\cite{DBLP:conf/ccs/RistenpartTSS09,DBLP:conf/sp/ZhangJOR11}.
This problem of side channel leakage has been extensively studied in the past by, and others have proposed correlation-based measures to quantify the amount of leakage~\cite{demme2012,zhang2013}.

In this paper, we consider the problem of efficiently learning and classifying the behavior of side channels through observed time series using probabilistic hashing techniques.
We formally define the probability of information leakage on a set of such time series traces observed at a fixed granularity with respect to a distance measure.
By expanding on that intuition, we propose a \textit{ranked leakage-sensitive hashing} scheme based on previous locality-sensitive hashing schemes~\cite{Kulis12-KLSH,Jiang15-KLSH,Kim16-SLSH} that exploits the probability of information leakage for nearest neighbor computations in anomaly detection and behavior classification.
We believe that these contributions lead to a novel, strong characterization of leakage over side channels, giving rise to a new notion of \textit{dimensionality} of a channel.
We show that this can determine the effectiveness of leakage prevention schemes as well as discuss possibilities of projecting onto higher and lower dimensional spaces to improve anomaly detection or improve the effectiveness of leakage prevention schemes.

\section{Background and Definitions}
\label{sec:definitions}
\label{sec:background}

Previous work in time series anomaly detection has largely focused on Euclidean distance and Dynamic Time Warping (DTW) distance~\cite{many,papers,by,those,guys}.
However, we find that there are drawbacks of limiting evaluation to these two measures only.
DTW is not a proper metric as it is not sub-additive; this is a result of DTW treating one time series as non-linear time-stretched of the other.
In the context of anomaly detection, this limits the ability to mark a small, unexpected change in behavior when compared with other distance measures.
Additionally, both of these measures operate under the assumption that observed time series which are similar will be of the same scale.
Two time series which exhibit similar ``behavior'' but take values of a slightly different scale will not be marked as being similar by either measure.
Changes in background system activity could therefore affect the ability of these two measure to detect true anomalous activity.

% Definition of leakage
% Hashing
% LSH hash family
% Anomaly detection-specific:
% Distance metric

\subsection{Information Leakage}
\label{subsec:information_leakage}
Side channel exploitation, anomaly detection, and covert channel communication are problems of detecting or exploiting leakage over information channels.
Side and covert channels exists when observable differences in system behavior occur as the result of actions performed by a \textit{victim} or sending process.
These attacks typically involve an adversary learning secret information over the channel, based on the behavior of a victim process.
Anomaly detection, on the other hand, involves detectors running on a system analyzing and categorizing observed behavior in real time.
In this setting, a malicious program leaks information about its behavior through an observed channel.

%To formally define information leakage and the ability to exploit or thwart leakage, 
We consider a single information channel as a sequence of observations of a system resource --- a \textit{time series} of resource observations.
For example, the \textit{trace} of system calls on a system over time is an $n$-dimensional time series, where each observation determines the number of times each of the $n$ system calls was invoked.

Our primary observation with regards to time series leakage is that information may only be learned from time series observations if the underlying distributions are distinguishable.

\begin{definition}[Distribution-based leakage]\label{defn:dist_leakage}
    Consider two distinct program behaviors $x$ and $x'$ and resulting time series for each behavior drawn from $D_x$ and $D_{x'}$ respectively.
    Let $t(x)$ and $t(x')$ be two time series resulting from behaviors $x, x'$, drawn from $D_x, D_{x'}$ respectively.
    Observing $t(x)$ and $t(x')$ can only leak information about $x$ and $x'$ if $D_x$, $D_{x'}$ are statistically distinguishable.
\end{definition}

While Definition~\ref{defn:dist_leakage} is useful when one can carefully observe many samples from different distributions to assess the distinguishability of the underlying distributions, it is difficult to use in practice.
Instead, we propose a slightly different definition of leakage:

\begin{definition}[Time series leakage]\label{defn:leakage}
    Consider two distinct program behaviors $x$ and $x'$ with output distributions $D_x$ and $D_{x'}$, and let $d(\cdot,\cdot)$ be a distance function on the output time series.
    Let $T_x = \{t_k\}$ be a sequence of time series drawn from $D_x$.
    Define $r$ as the minimum distance such that $d(t_i, t_j) \le r~\forall~t_i,t_j \in T_x$

    We say that observing time series $t(x)$ and $t(x')$ can \textbf{leak information with respect to $d(\cdot,\cdot)$} about $x$ and $x'$ if $d(t(x),t(x')) > r$.
\end{definition}

Definition~\ref{defn:leakage} describes time series leakage with respect to a specific distance function applied on observation points.
If time series resulting from behavior $x$ can be separated from time series from $x'$ by a distance of more than $r$, there is potential information leakage through this channel.

\subsection{Hashing}
\label{subsec:hashing}
Hashing has long been used as a method of easing the curse of dimensionality for tasks such as clustering on a large set of high-dimensional data~\cite{Indyk98-ANN,Gionis99-SSH,Datar04-LSH}.
Exploiting the probabilistic nature and the computational efficiency of hashing enables approximations to difficult high-dimensional problems quickly and in real-time.

Consider the space of time series $S$ and a distance function $d$ on $S$.
A LSH family is defined as such:

\begin{definition}[Hash family]\label{defn:hash_family}
    A hash family ${\LSH} = \{ h : S \rightarrow U \}$ is called $(r_1, r_2, p_1, p_2)$-sensitive w.r.t. $d(\cdot,\cdot)$ if for any $x,y \in S, h \in {\LSH}$
    \begin{enumerate}[(i)]
        \item\label{itm:hash_def1} If $d(x,y) \le r_1$, then $\Pr[h(x) = h(y)] \ge p_1$
        \item\label{itm:hash_def2} If $d(x,y) \ge r_2$, then $\Pr[h(x) = h(y)] \le p_2$
    \end{enumerate}
\end{definition}

Such a family is only interesting if $p_1 > p_2$.
To increase the effectiveness of an LSH technique, the gap between $p_1$ and $p_2$ may be \textit{amplified}:

\begin{definition}[LSH amplification]\label{defn:lsh_amplification}
    Consider a $(r_1, r_2, p_1, p_2)$-sensitive hash family ${\LSH}$ w.r.t $d(\cdot,\cdot)$. The LSH hash family can be amplified in the following ways:
    \begin{enumerate}[(i)]
        \item \textbf{AND construction:} Define ${\LSH}' = \{h' : S \rightarrow U^r\}$ such that $h' = [h_1, \ldots, h_r] \subset {\LSH}$.
            $h'(x) = h'(y)$ iff $h_i(x) = h_i(y) ~\forall~h_i \in h'$
            $\mathcal{H'}$ is a $(r_1, r_2, p_1^r, p_2^r)$-sensitive LSH family
        \item \textbf{OR construction:} Define ${\LSH}' = \{h' : S \rightarrow U^b\}$ such that $h' = [h_1, \ldots, h_b] \subset {\LSH}$.
            $h'(x) = h'(y)$ iff $h_i(x) = h_i(y)$ for any $h_i \in h'$
            $\mathcal{H'}$ is a $(r_1, r_2, 1-{(1-p_1)}^b, 1-{(1-p_2)}^b)$-sensitive LSH family
        \item \textbf{AND-OR composition:} The composition of \textnormal{and} with \textnormal{or} constructions defines a $(r_1, r_2, 1-{(1-p_1^r)}^b, 1-{(1-p_2^r)}^b)$-sensitive LSH family
    \end{enumerate}
\end{definition}

We consider hash families ${\LSH} = \{h : S \rightarrow \R_{+}^n\}$ such that $h \in {\LSH}$ approximates $d(\cdot,\cdot)$ on time series in $S$.
Intuitively, time series which are ``closer'' to each other (as defined by $d$) will be harder to distinguish by any arbitrary classifier and thus present fewer possibilities for information leakage.

% Maybe move this to a background section.
\subsection{Distance Measure}
Denote the space of a time series as $S$, and define a function $d$:

\begin{align*}
    d \colon S^2 &\to \R_{+}\\
    (x,y) &\mapsto r \numberthis
\end{align*}

where $d$ maps two time series to a non-negative real number that represents some notion of distance between them.
Additionally, we may require $d$ to be sub-additive:

\begin{align*}
    d(x,y) \le d(x,z) + d(z,y) \numberthis
\end{align*}

% What do we need from this metric?
The function $d(\cdot,\cdot)$ defines leakage in our threat model.
To give intuition behind this, we consider an arbitrary classification attack on a set of time series.
Applying Definitions~\ref{defn:dist_leakage} and~\ref{defn:leakage}, there is potential for leakage by observing the resulting time series if, for some function $d$, there exists separation by $d(\cdot, \cdot)$ between time series of differing classes.

% Why triangle inequality is important for ranking

% Suppose d(x,y) > r1 but h(x) = h(y)
% Then: let z = proj(y) onto B(r,x)
% d(x,z) = r1
% Then d(x,y) \le d(x,z) + d(z,y)
% Then wat

% Discussion of time series comparison for leakage, in general

\subsection{Kernel Transforms}
\label{subsec:kernel_transforms}

We have so far defined leakage with respect to an arbitrary, but fixed, distance measure.
However, we now consider kernel transforms to define higher dimensional distance measures without explicitly defining the embedding space~\cite{Scholkopf00}.
This methodology allows us to determine a leakage-sensitive distance measure with computational efficiency.

Kernel transforms have been used extensively in machine learning problems, especially in support vector machine (SVM) classifiers. 
For example, a kernel transform allows the use of user-specified similarity functions that may be computationally intractable to fully define.
However, kernel transforms have also been recently applied to hashing problems in order to tackle even higher dimensional similarity problems~\cite{Kulis12-KLSH, Kale14-KLSH, Jiang15-KLSH}.
A kernel function $\kappa(\cdot,\cdot)$ thus defines a new similarity measure on a higher dimensional space over which we would not otherwise be able to efficiently hash.

The following definitions let us formally define kernel transforms on time series:

\begin{definition}[Hilbert space]\label{defn:hilbert_space}
    A vector space $H$ over a field $F$ with an inner product $\innerproduct{\cdot}{\cdot}_H: H \times H \rightarrow F$ that also defines a complete\footnote{A space $X$ is complete if every Cauchy sequence  converges in $X$. A Cauchy sequence is a sequence $\{x_n\}_{n \in \N}, x_n \in X$ with $\lim\limits_{(m,n)\rightarrow\infty} |x_m - x_n| = 0$.} metric space is called a Hilbert space.
\end{definition}

The key property of Hilbert spaces we wish to leverage is the norm induced by the inner product $\innerproduct{\cdot}{\cdot}_H$.
This inner product defines the higher order distance measure we wish to use on the raw observation space.

\begin{definition}[Kernel transform]\label{defn:kernel_trick}
    Let $X$ be an arbitrary space and $H$ be a Hilbert space with inner product $\innerproduct{\cdot}{\cdot}_H$.
    $\kappa(\cdot, \cdot): X \times X \rightarrow \R$ is a kernel transform if $\kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H$ for some $\phi: X \rightarrow H$.
\end{definition}

Note that in Definition~\ref{defn:kernel_trick}, the mapping function $\phi$ need not be explicitly defined.
In fact, $\kappa$ being a positive-semidefinite function (or matrix over discrete spaces) implies the existence of a satisfactory function $\phi$.
Thus, we can consider arbitrary higher-order distance measures as any positive-semidefinite mapping $\kappa: X \times X \rightarrow H$ guarantees a similarity measure in $H$.

\begin{definition}[Reproducing kernel Hilbert space (RKHS)]\label{defn:rkhs}
    Let $H$ be a Hilbert space of real-valued functions on an arbitrary set $X$.
    $H$ is a reproducing kernel Hilbert space if there exists a \textbf{reproducing kernel}, $\kappa_x$ $\forall x \in X$, where $f(x) = \innerproduct{f}{\kappa_x}_H~\forall~f \in H$.
\end{definition}

Note that $\kappa(x,y) = \innerproduct{\kappa_x}{\kappa_y}_H$, and thus the kernel transform in Definition~\ref{defn:kernel_trick} defines a RKHS.
This demonstrates that we can consider arbitrary higher-order similarity measures using kernel functions on the space of observed samples.
Furthermore, we can construct proper distance measures from the norm induced by the inner product on a RKHS.

\begin{definition}[Hilbert norm-induced distance]\label{defn:norm_distance}
    Given an input set $X$ and a reproducing kernel $\kappa$ for RKHS $H$: $\kappa(\cdot, \cdot) \colon X \times X \rightarrow \R$, define a distance measure on $X$ by:
    \begin{align*}
        d(x,y)^2 &= \norm{\phi(x) - \phi(y)}_H^2 = \innerproduct{\phi(x) - \phi(y)}{\phi(x) - \phi(y)}_H \\
        &= \innerproduct{z}{z}_H = \kappa(\phi^{-1}(z), \phi^{-1}(z)) \numberthis
    \end{align*}
\end{definition}

Note that we have still not explicitly defined the feature map $\phi(\cdot)$.
Instead, we contend that such a function exists that allows our transform to take place, which we prove in Section~\ref{subsec:proposed_kernel}.

\section{Kernelized Hashing Model for Time Series}
\label{sec:kernel_hashing}

We now propose a specific hashing model for time series with the goal of anomaly detection.

\begin{definition}[Property $\mathscr{A}$]\label{defn:property_alpha}
    Let $\LSH$ be a $(2r, s, p, q)$-sensitive LSH family on $X$ with distance measure $d(\cdot, \cdot)$.
    $\LSH$ has property~$\mathscr{A}$ if the following holds:
    Fix $x \in X$ and construct $S = \{s \in X \mid d(x,s) \le r\}$.
    Then, $\Pr[h(x_i) = h(x_j)] \ge p~\forall x_i, x_j \in S, h \in \LSH$ 
\end{definition}

Property~$\mathscr{A}$ confers the notion that elements which lie within a ball of fixed radius should have high probability of hashing to the same value.
Furthermore, such families may be \textit{nested} with increasing values of $n$ to form neighborhoods with different levels of similarity.
We now define what we need from a distance measure $d$ to obtain this nesting property.

\begin{claim}\label{prop:property_alpha}
    An LSH family $\LSH$ with distance measure $d(\cdot, \cdot)$ on a set $X$ has Property~$\mathscr{A}$ if $d(\cdot, \cdot)$ is a sub-additive distance measure.
\end{claim}

\begin{proof}
    Let $\LSH$ be an LSH family on $X$ which is $(2r, s, p, q)$-sensitive and suppose $d: X \times X \rightarrow \R_+$ is sub-additive.

    Fix $x \in X$ and construct $S = \{s \in X \mid d(x,s) \le r\}$.
    Since $d$ is a sub-additive distance measure, $d(x_i, x_j) \le d(x_i,x) + d(x,x_j) = d(x,x_i) + d(x,x_j) \le 2r~\forall x_i,x_j \in S$.
    Thus by construction of $\LSH$, $\Pr[h(x_i) = h(x_j)] \ge p~\forall x_i, x_j \in S, h \in \LSH$, and thus $\LSH$ has property~$\mathscr{A}$.
\end{proof}

Property $\mathscr{A}$ additionally allows for a stratified LSH scheme using a set of LSH families index by a distance $r$ which allows us to confer a notion of closeness between buckets of a lower LSH strata.

\begin{definition}[Ranked LSH families]\label{defn:ranked_lsh}
    A set of hash families ${\{{\LSH}_r\}}_{r\in R}$ is a ranked LSH family $H_r$ has property $\mathscr{A}~\forall r \in R$.
    Denote such a set of families as $(R,p_1, p_2)$-sensitive LSH families ${\LSH}_R$.
    %, where $p = \inf\{ p_i | H_{r_1}$ is $(r_1,r_1,p_i,p_i)$-sensitive $\}$.
\end{definition}

% TODO: is this proposition necessary?
%In summary, we are now considering a \textit{ranked LSH family} with the following properties:
%\begin{proposition}\label{prop:ranked_lsh}
%    There exists $(R, p_1, p_2)$-sensitive ranked LSH families ${\LSH}_R = {\{{\LSH}_r\}}_{r\in R}$ such that the following hold:
%    \begin{enumerate}[(i)]
%        \item\label{itm:first}
%            $h \in {\LSH}_r \in {\LSH}_R$ collides with high probability if $y \in B_r(x)$: If $d(x,y) < r, \Pr[h(x) = h(y)] \ge p_1$.
%        \item\label{itm:second}
%            $h \in {\LSH}_r$ has few false collisions: If $d(x,y) > r$, $\Pr[h(x) = h(y)] < p_2$.
%        \item\label{itm:rank_property}
%            Property $\mathscr{A}$ holds for any two families ${\LSH}_{r_1}, {\LSH}_{r_2} \in {\LSH}_R$.
%    \end{enumerate}
%\end{proposition}

%\begin{proof}
%\end{proof}

This corroborates the notion of closeness to collision probabilities, which allows for the grouping of similar time series.
Due to Property~$\mathscr{A}$, we may apply iterative hashing scheme to \textit{rank} the probabilities of closeness based on the varied parameter $r$.
We discuss this in greater depth in Section~\ref{subsec:ranked_lsh}

\subsection{Kernel Model}
\label{subsec:kernel_model}

Consider a set of $n$ samples $D = \{x_1,\ldots,x_n\} \subset \R^d$ with similarity measure $\kappa \colon D \times D \rightarrow \R$ defined by $\kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H$, with $H$ being a RKHS.
We now consider the effects of a positive-semidefinite similarity measure $\kappa(\cdot, \cdot)$ as well as its corresponding distance measure $\tilde{\kappa}(x,y) = \|\phi(x) - \phi(y)\|_H$, the norm induced by the RKHS.
Note that from Definition~\ref{defn:kernel_trick}, we do not need to explicitly define the mapping $\phi(\cdot)$ to an RKHS.

\begin{definition}[Kernelized LSH]\label{defn:kernel_lsh}
    %Let $\kappa_r(\cdot, \cdot) = \kappa()$ 
    ${\LSH}_R = \{ {\LSH}_r \}_{r \in R}$ is a $(R, p, q)$-sensitive ranked, kernelized LSH family if, for any $x,y \in D$:
    \begin{enumerate}[(i)]
        \item If $\tilde{\kappa}(x,y) \le r, \Pr[h_r(x) = h_r(y)] > p$
        \item If $\tilde{\kappa}(x,y) \ge r, \Pr[h_r(x) = h_r(y)] < q$
        \item Property $\mathscr{A}$ holds for any LSH family ${\LSH}_{r_i} \in {\LSH}_R$
    \end{enumerate}
    Denote such a family a \textbf{RKLSH} family.
\end{definition}

We now consider how to apply a RKLSH family to time series for anomaly detection and construct such a family in Section~\ref{sec:proposed_scheme}.

\subsection{Ranked Hashing by Iteration}
\label{subsec:ranked_lsh}

Let ${\LSH}_R$ be a RKLSH family which approximates a measure on time series distance with $R$ being a set of stratifying distance thresholds, and let $S$ be the space of all possible time series.

% You can define a relation wrt a single point, for each point and for each threshold
% Triangulate based on which two fall into same class at each threshold
% Intuitively, probabilistic neighborhoods defined for < r_1 threshold
% < r_2 should confirm all of the < r_1 with high probability, forming a tier 2 neighborhood
% Two neighborhoods are ``ranked'' based on the lowest tier on which they reside in the same neighborhood

\begin{definition}[LSH neighborhood]\label{defn:neighborhood}
    A \textbf{neighborhood} with respect to distance threshold $r_i \in R$ and LSH hash function $h \in {\LSH}_R$} is a set of points $S$ such that $\forall s_i, s_j \in S, h(s_i) = h(s_j)$.
\end{definition}

The goal of hashing the set of input points is to efficiently compute sets of \textit{approximate nearest neighbors} (ANN) such that the behavior of each point can be classified.
However, the construction of RKLSH we have previously defined provides extra structure which allows us to make even stronger similarity claims:

\begin{definition}[Neighborhood rank]\label{defn:neighborhood_rank}
    Let $X$ be an input space and ${\LSH}_R$ an RKLSH, with $R = \{r_1, r_2, \ldots\}$.
    Two data samples $x,y \in X$ have \textbf{rank} $n$ if $n = \inf\limits_i\{ r_i \in R \mid \forall r_j \ge r_i,~\exists h_{r_j} \in {\LSH}_{r_j}$ such that $h_{r_j}(x) = h_{r_j}(y) \}$.
\end{definition}

We can thus apply the ranked hashing scheme to perform a stronger approximate nearest-neighbor calculation than with a single LSH family.
The similarity of any two points within a neighborhood can be ranked to give an indication of the confidence a point truly falls within a given neighborhood and thus exhibits the given behavior.

Thus, by applying this tiered ANN calculation, we are able to \textit{rank} the similarity of two elements by examining the threshold at which their hashes collide.

\subsection{Anomaly Detection on time series}
\label{subsec:anomaly_detection}

What this theory describes is that similar time series may be grouped together and ranked by their closeness via a kernel distance measure $\tilde{\kappa}$~\cite{Hachiya13-NSH}.
In the context of anomaly detection, we can apply this scheme to a set of \textit{normal} traces (time series of utilization, label encoded syscalls, etc.) to determine what thresholds and clusters constitute normal execution with a finer granularity.
Instead of a binary label of \textit{normal} vs \textit{anomalous}, we can instead stratify applications by types of execution.
We speculate that this makes our detection algorithm more robust to adversarial interference.

Given the initial LSH clustering of normal traces, we can continuously apply the hashing scheme to testing traces over a rolling window to categorize their behavior.
Traces which do not match enough previously ``normal'' clusters over the set of thresholds may be considered to represent anomalous activity.

The streams of time series our LSH algorithm considers have indeterminate length, so we consider fixed fixed observation periods and hash time series segments.
However, performing this real-time anomaly detection still leaves the issue of a malicious program changing behavior to match different benign programs during different observation periods.
Unless such behavioral changes were present in the original training set, this behavior should be considered anomalous.

To combat behavioral changes in malicious programs, we randomize the observation periods for each distance threshold $r_i$ in the LSH algorithm.
This guarantees that such behavior changes will be observed during the hashing process for some distance thresholds.

\iffalse
\subsection{Unsupervised, Intelligent Adversary}

A similar approach to Section~\ref{subsec:anomaly_detection} can be applied without a predetermined training set for time series classification through clustering.
Adjusting the set of threshold values confers the notion of closeness, which may be used to determine which sets of output traces resulted from the same input to a program, for example.
\fi

\section{Seminorms in Hilbert Spaces}
\label{sec:sobolev}

We have so far defined machinery which will, given a reproducing kernel $\kappa$ from the observation space, permit us to calculate higher order distances using the implicit feature map $\phi$ induced by the kernel function.
Recall the kernel transform:

\begin{align}
    \kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H
\end{align}

From this, the induced distance measure is:

\begin{align}
    d(x,y) = \norm{\phi(x) - \phi(y)}_H
\end{align}

However, we may wish to understand behavior even more explicitly in this space by considering a functions derivatives.
To this end, we consider the concept of \textit{seminorms}.

\begin{definition}[Seminorm]
    A seminorm $\norm{\cdot}_S$ on a space $S$ satisfies:
    \begin{enumerate}[(i)]
        \item $\norm{ax}_S = |a|\norm{x}_S$ for scalar $a$
        \item $\norm{x + y}_S \le \norm{x}_S + \norm{y}_S$ (triangle inequality)
    \end{enumerate}
\end{definition}

This has some benefits for time series classification which we seek to exploit.
\begin{itemize}
    \item \textbf{Shift-agnostic:} Let $f,g \in H$, a RKHS, and let $g = f + c$, where $c$ is a constant, and let both $f,g$ be $n$-differentiable.
        The norms $\norm{f}_H, \norm{g}_H$ will vary drastically depending on $c$.
        However, such shifting is not apparent with a derivative-based seminorm: $\norm{x}_S = {(\sum\limits_{k=1}^n \norm{D^k x}^p_H)}^{\frac{1}{p}}$.
        Thus $\norm{g}_S = {(\sum\limits_{k=1}^n \norm{D^k (f+c)}^p_H)}^{\frac{1}{p}} = {(\sum\limits_{k=1}^n \norm{D^k f}^p_H)}^{\frac{1}{p}} = \norm{f}_S$.
        Here, the operator $D$ is understood in a weak sense (see Section~\ref{subsec:weak_derivatives}).
    \item \textbf{Scaling behavior:} Consider the seminorm $\norm{x}_S = {\sum\limits_{|\alpha| = 1}^n(\norm{D^{\alpha} x}^p_H)}^{\frac{1}{p}}$.
        Let $f,g \in H$ be at least once differentiable with $g(x) = f(cx)$, $c$ a scalar.
        Then, $\norm{g(x)}_S = \norm{f(cx)}_S = {\sum\limits_{|\alpha| = 1}^n(\norm{D^{\alpha} f(cx)}^p_H)}^{\frac{1}{p}} = r^{p-n} \norm{f(x)}_S$ where $n$ is the dimension of $H$.
\end{itemize}

Expanding on Definition~\ref{defn:norm_distance} to create a distance metric based on a seminorm, we consider the following derivative-based distance measure:

\begin{align}
    d(x,y) = \norm{\phi(x)-\phi(y)}_S = \norm{D (\phi(x) - \phi(y))}_H = \norm{\phi'(x) - \phi'(y)}_H
\end{align}

If we are able to impose additional structure onto the feature map $\phi(\cdot)$, the seminorm definition has much greater value.

Suppose $\phi \colon X \rightarrow H$ is a feature map from input space $X$ to Hilbert space $H$, and that $\phi$ is surjective and once differentiable.
Then, $\exists z$ such that $\phi'(x) - \phi'(y) = \phi(z)$ since $H$ is a vector space closed under addition, and $\phi$ is surjective.

\begin{align}
    d(x,y) = \norm{\phi'(x) - \phi'(y)}_H = \norm{\phi(z)}_H = \sqrt{\innerproduct{\phi(z)}{\phi(z)}_H} = \sqrt{\kappa_H(z,z)}
\end{align}

where $\kappa_H$ is the reproducing kernel of $H$.

Note that while $d(\cdot,\cdot)$ as defined above is sub-additive~\cite{rosenlicht68-realanalysis}, it is not identically zero and is thus not a proper metric: $g = f + c$ meets $d(f,g) = 0$ even when $f \neq g$.

Given a ``nice'' feature mapping $\phi$ from input space to a Hilbert space induced by a reproducing kernel, we can more intuitively and effectively categorize time series behavior and time series distance using seminorms in these spaces.
However, this is not trivial, as Mercer's condition and theorem only guarantee the existence of $\phi$ given a positive semi-definite kernel and make no guarantees on its differentiability or surjectivity.
We explore the potential for seminorm usage in the next section with an explicit kernel for anomaly detection.

\section{Proposed Scheme}
\label{sec:proposed_scheme}

We now propose a specific distance measure and kernel transform to maximize the impact of our RKLSH algorithm for anomaly detection.

\subsection{Dynamic Time Warping}

% Empirical evaluation of DTW on traces *for anomaly detection*

\subsection{Minkowski-based Measures}

Consider the Minkowski distance:

\begin{align}
    M(x,y) = {(\sum\limits_k |x_k - y_k|^p)}^{1/p}
\end{align}

For $p \ge 1$, $M(\cdot,\cdot)$ is a measure due to Minkowski's inequality~\cite[p. 190]{wheeden15-measure}.
Additionally, for $p = 2$, this is the standard Euclidean distance.
However, like the Euclidean distance, this distance metric fails to account for phase and shape changes between time series.

To this end, we consider two different measures based on a $p$-dimensional Minkowski distance.
The first is due to Batista et al.~\cite{batista14-cid} which attempts to account for time series complexity and has been shown to greatly improve the mean accuracy rates of time series comparison~\cite{giusti13-ecd}.
Consider the following \textit{complexity measure}:

\begin{align*}
    C(x) = \sqrt{\sum\limits_i {(q_i - q_{i+1})}^2} \numberthis
\end{align*}

Using this complexity measure, we define a \textit{complexity-invariant} distance measure based on the Minkowski distance $M(\cdot,\cdot)$:

\begin{align*}
    d(x,y) = M(x,y) * \frac{\max(C(x),C(y))}{\min(C(x),C(y))} \numberthis
\end{align*}

The second measure we consider attempts to correlate Minkowski distance across time with an additional penalty term for time series which are out of phase:

\begin{align*}
    d(x,y) = \min\limits_{i,j<\frac{n}{2}} \{ M(x[i:N],y[j:N]) + c*(i+j) \} \numberthis \\
\end{align*}

where $x,y$ have length $n$ and $N = n - \max(i,j)$.
$c$ is a scalar penalty factor for out of phase alignment between $x$ and $y$.

For both of these measures, we consider $p$-dimensional Minkowski distance where the original time series have length $p$.

\subsection{Proposed Hash}

Let $S$ be the space of $n$-point time series to be processed.
Pick $a$ random lines in $\R^2$ and project each point $x_i$ onto each line $h_k$.
Each line will be partitioned into buckets of size $\frac{r}{n}$ (here, we see the parametrization of the hash).
In practice, we fix a number $M$ (say, $2^{32}$), and apply $h_r(x) = \frac{\pi(x)*n}{r} \bmod M$. 

Points are considered to intersect if they hash to the same bucket on a fixed number of hash function.
We propose that the number of hash functions on which points must collide to determine intersection be a tunable parameter.
Time series which intersect on more than $\frac{n}{2}$ points (in time order) are said to be candidate pairs for $r$-closeness.

\section{Key Takeaways}

\begin{itemize}
    \item Use hash that approximates time series distance.
    \item Compose multiple kernels or hash families to obtain ranked ``normality'' measure.
    \item Apply ranking to classification by an unsupervised classifier to determine amount of leakage possible without prior knowledge.
    \item Apply ranking to normality (anomaly) detection.
    \item Perform continuous hashing on rolling windows of execution across channels of multiple resources to classify activity in real-time.
    \item To be considered: Overlay results with ShapeGD\@.
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{../bibliography/bibliography}

\end{document}
