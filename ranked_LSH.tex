\documentclass[a4paper]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{parskip,amsmath,mathtools,amssymb,amsfonts,mathrsfs}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{url,titling}
\usepackage{fancyhdr,hyperref}
\usepackage{booktabs,color,tabularx}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{bm}
\usepackage{float}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\cardinality}[1]{\left\lvert#1\right\rvert}
\newcommand{\innerproduct}[2]{\langle{}#1,#2\rangle{}}
\newtheoremstyle{def}
{8pt}
{5pt}
{}
{}
{\bfseries}
{:}
{.5em}
{}

\newtheoremstyle{thm}
{8pt}
{5pt}
{\itshape\/}
{}
{\bfseries}
{:}
{.5em}
{}

\theoremstyle{def}
\newtheorem{definition}{Definition}
\newtheorem{assumption}[definition]{Assumption}
\theoremstyle{thm}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{claim}[proposition]{Claim}
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}{Corollary}[proposition]
%\newenvironment{proof}[1][]{\textbf{Proof:} }{\hfill$\square$}

%\setlength{\textfloatsep}{6pt}

% Math symbols
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\newcommand{\A}[0]{\mathcal{A}}
\newcommand{\e}[1]{\text{e}^{#1}}
\newcommand{\E}[1]{\mathbf{E}\left[#1\right]}
\newcommand{\p}[0]{\mathbf{p}}
\newcommand{\q}[0]{\mathbf{q}}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\LSH}[0]{\mathcal{H}}
\newcommand{\re}[1]{\frac{1}{#1}}
\newcommand{\set}[1]{\lbrace\ 0,1 \rbrace^{#1}}
\newcommand{\X}[0]{\mathcal{X}}
\newcommand{\Y}[0]{\mathcal{Y}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\aand}[0]{\text{ and }}
\newcommand{\iif}[0]{\text{ if }}
\newcommand{\ow}[0]{\text{ otherwise}}

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO\@: {#1}}}

\setlength{\droptitle}{-8em}
\title{Behavior Sensitive Hashing}
\author{Rohith Prakash}
\date{}

\begin{document}
\maketitle{}

\begin{abstract}
    In this paper, we study the problem of real-time anomaly detection of intelligent malware samples disguised within benign applications. 
    We demonstrate that commonly used Dynamic Time Warping (DTW) distance is not suitable on time series of system resource traces when malware samples dynamically adapt their behavior to evade detection.
    To deal with malware samples in real-time which attempt to hide within benign behavior, we propose a new LSH-based scheme that has low hardware and complexity overhead, iteratively hashes time series based on \textit{behavioral patterns}, triangulates hashing buckets to better categorize behavior, and is able to categorize new, previously unobserved behavior.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Time series are used almost ubiquitously to represent time-based measurements in various fields.
However, it is a well known problem that when these times series represent behavior-related observations of complex systems, unintended information about the system may be leaked through this channel~\cite{DBLP:conf/ccs/RistenpartTSS09,DBLP:conf/sp/ZhangJOR11}.
This problem of side channel leakage has been extensively studied in the past by, and others have proposed correlation-based measures to quantify the amount of leakage~\cite{demme2012,zhang2013}.

In this paper, we consider the problem of efficiently learning and classifying the behavior of side channels through observed time series using probabilistic hashing techniques.
We formally define the probability of information leakage on a set of such time series traces observed at a fixed granularity with respect to a distance measure.
By expanding on that intuition, we propose a \textit{ranked behavior-sensitive hashing} scheme based on previous locality-sensitive hashing schemes~\cite{Kulis12-KLSH,Jiang15-KLSH,Kim16-SLSH} that exploits the probability of information leakage for nearest neighbor computations in anomaly detection and behavior classification.
We believe that these contributions lead to a novel, strong characterization of leakage over side channels, giving rise to a new notion of \textit{dimensionality} of a channel.
We show that this can determine the effectiveness of leakage prevention schemes as well as discuss possibilities of projecting onto higher and lower dimensional spaces to improve anomaly detection or improve the effectiveness of leakage prevention schemes.

\section{Background and Definitions}
\label{sec:definitions}
\label{sec:background}

Previous work in time series anomaly detection has largely focused on Euclidean distance and Dynamic Time Warping (DTW) distance~\cite{many,papers,by,those,guys}.
However, we find that there are drawbacks of limiting evaluation to these two measures only.
DTW is not a proper metric as it is not sub-additive; this is a result of DTW treating one time series as non-linear time-stretched of the other.
In the context of anomaly detection, this limits the ability to mark a small, unexpected change in behavior when compared with other distance measures.
Additionally, both of these measures operate under the assumption that observed time series which are similar will be of the same scale.
Two time series which exhibit similar ``behavior'' but take values of a slightly different scale will not be marked as being similar by either measure.
Changes in background system activity could therefore affect the ability of these two measure to detect true anomalous activity.

% Definition of leakage
% Hashing
% LSH hash family
% Anomaly detection-specific:
% Distance metric

\subsection{Information Leakage}
\label{subsec:information_leakage}
Side channel exploitation, anomaly detection, and covert channel communication are problems of detecting or exploiting leakage over information channels.
Side and covert channels exists when observable differences in system behavior occur as the result of actions performed by a \textit{victim} or sending process.
These attacks typically involve an adversary learning secret information over the channel, based on the behavior of a victim process.
Anomaly detection, on the other hand, involves detectors running on a system analyzing and categorizing observed behavior in real time.
In this setting, a malicious program leaks information about its behavior through an observed channel.

%To formally define information leakage and the ability to exploit or thwart leakage, 
We consider a single information channel as a sequence of observations of a system resource --- a \textit{time series} of resource observations.
For example, the \textit{trace} of system calls on a system over time is an $n$-dimensional time series, where each observation determines the number of times each of the $n$ system calls was invoked.

Our primary observation with regards to time series leakage is that information may only be learned from time series observations if the underlying distributions are distinguishable.

\begin{definition}[Distribution-based behavior leakage]\label{defn:dist_leakage}
    Consider two distinct program behaviors $x$ and $x'$ and resulting time series for each behavior drawn from $D_x$ and $D_{x'}$ respectively.
    Let $t(x)$ and $t(x')$ be two time series resulting from behaviors $x, x'$, drawn from $D_x, D_{x'}$ respectively.
    Observing $t(x)$ and $t(x')$ can only leak information about $x$ and $x'$ if $D_x$, $D_{x'}$ are statistically distinguishable.
\end{definition}

While Definition~\ref{defn:dist_leakage} is useful when one can carefully observe many samples from different distributions to assess the distinguishability of the underlying distributions, it is difficult to use in practice.
Instead, we propose a slightly different definition of behavior dissimilarity:

\begin{definition}[Time series behavior leakage]\label{defn:leakage}
    Consider two distinct program behaviors $x$ and $x'$ with output distributions $D_x$ and $D_{x'}$.
    
    We say that program behaviors $x$ and $x'$ may \textbf{leak behavior information} regarding $x$ and $x'$ if there exists a distance measure $d(\cdot, \cdot)$ such that $\forall~t_{x_i}, t_{x_j} \sim D_x$ and $t_{x'_i}, t_{x'_j} \sim D_{x'}$, the following hold:
    \begin{align*}
        \mathbf{E}[d(t_{x_i}, t_{x'_i})] > \mathbf{E}[d(t_{x_i}, t_{x_j})],\\
        \mathbf{E}[d(t_{x_i}, t_{x'_i})] > \mathbf{E}[d(t_{x'_i}, t_{x'_j})]. \numberthis
    \end{align*}

\end{definition}

Definition~\ref{defn:leakage} describes time series behavior with respect to a specific distance function applied on observation points.
If time series resulting from behavior $x$ can be separated from time series from $x'$ by a distance of more than $r$, there is potential information leakage through this channel.

\subsection{Hashing}
\label{subsec:hashing}
Hashing has long been used as a method of easing the curse of dimensionality for tasks such as clustering on a large set of high-dimensional data~\cite{Indyk98-ANN,Gionis99-SSH,Datar04-LSH}.
Exploiting the probabilistic nature and the computational efficiency of hashing enables approximations to difficult high-dimensional problems quickly and in real-time.

Consider the space of time series $S$ and a distance function $d$ on $S$.
A LSH family is defined as such:

\begin{definition}[Hash family]\label{defn:hash_family}
    A hash family ${\LSH} = \{ h : S \rightarrow U \}$ is called $(r_1, r_2, p_1, p_2)$-sensitive w.r.t. $d(\cdot,\cdot)$ if for any $x,y \in S, h \in {\LSH}$
    \begin{enumerate}[(i)]
        \item\label{itm:hash_def1} If $d(x,y) \le r_1$, then $\Pr[h(x) = h(y)] \ge p_1$
        \item\label{itm:hash_def2} If $d(x,y) \ge r_2$, then $\Pr[h(x) = h(y)] \le p_2$
    \end{enumerate}
\end{definition}

Such a family is only interesting if $p_1 > p_2$.
To increase the effectiveness of an LSH technique, the gap between $p_1$ and $p_2$ may be \textit{amplified}:

\begin{definition}[LSH amplification]\label{defn:lsh_amplification}
    Consider a $(r_1, r_2, p_1, p_2)$-sensitive hash family ${\LSH}$ w.r.t $d(\cdot,\cdot)$. The LSH hash family can be amplified in the following ways:
    \begin{enumerate}[(i)]
        \item \textbf{AND construction:} Define ${\LSH}' = \{h' : S \rightarrow U^r\}$ such that $h' = [h_1, \ldots, h_r] \subset {\LSH}$.
            $h'(x) = h'(y)$ iff $h_i(x) = h_i(y) ~\forall~h_i \in h'$
            $\mathcal{H'}$ is a $(r_1, r_2, p_1^r, p_2^r)$-sensitive LSH family
        \item \textbf{OR construction:} Define ${\LSH}' = \{h' : S \rightarrow U^b\}$ such that $h' = [h_1, \ldots, h_b] \subset {\LSH}$.
            $h'(x) = h'(y)$ iff $h_i(x) = h_i(y)$ for any $h_i \in h'$
            $\mathcal{H'}$ is a $(r_1, r_2, 1-{(1-p_1)}^b, 1-{(1-p_2)}^b)$-sensitive LSH family
        \item \textbf{AND-OR composition:} The composition of \textnormal{and} with \textnormal{or} constructions defines a $(r_1, r_2, 1-{(1-p_1^r)}^b, 1-{(1-p_2^r)}^b)$-sensitive LSH family
    \end{enumerate}
\end{definition}

We consider hash families ${\LSH} = \{h : S \rightarrow \R_{+}^n\}$ such that $h \in {\LSH}$ approximates $d(\cdot,\cdot)$ on time series in $S$.
Intuitively, time series which are ``closer'' to each other (as defined by $d$) will be harder to distinguish by any arbitrary classifier and thus present fewer possibilities for information leakage.

% Maybe move this to a background section.
\subsection{Distance Measure}
Denote the space of a time series as $S$, and define a function $d$:

\begin{align*}
    d \colon S^2 &\to \R_{+}\\
    (x,y) &\mapsto r \numberthis
\end{align*}

where $d$ maps two time series to a non-negative real number that represents some notion of distance between them.
Additionally, we may require $d$ to be sub-additive:

\begin{align*}
    d(x,y) \le d(x,z) + d(z,y) \numberthis
\end{align*}

% What do we need from this metric?
The function $d(\cdot,\cdot)$ defines leakage in our threat model.
To give intuition behind this, we consider an arbitrary classification attack on a set of time series.
Applying Definitions~\ref{defn:dist_leakage} and~\ref{defn:leakage}, there is potential for leakage by observing the resulting time series if, for some function $d$, there exists separation by $d(\cdot, \cdot)$ between time series of differing classes.

% Why triangle inequality is important for ranking

% Suppose d(x,y) > r1 but h(x) = h(y)
% Then: let z = proj(y) onto B(r,x)
% d(x,z) = r1
% Then d(x,y) \le d(x,z) + d(z,y)
% Then wat

% Discussion of time series comparison for leakage, in general

\subsection{Kernel Transforms}
\label{subsec:kernel_transforms}

We have so far defined leakage with respect to an arbitrary, but fixed, distance measure.
However, we now consider kernel transforms to define higher dimensional distance measures without explicitly defining the embedding space~\cite{Scholkopf00}.
This methodology allows us to determine a leakage-sensitive distance measure with computational efficiency.

Kernel transforms have been used extensively in machine learning problems, especially in support vector machine (SVM) classifiers. 
For example, a kernel transform allows the use of user-specified similarity functions that may be computationally intractable to fully define.
However, kernel transforms have also been recently applied to hashing problems in order to tackle even higher dimensional similarity problems~\cite{Kulis12-KLSH, Kale14-KLSH, Jiang15-KLSH}.
A kernel function $\kappa(\cdot,\cdot)$ thus defines a new similarity measure on a higher dimensional space over which we would not otherwise be able to efficiently hash.

The following definitions let us formally define kernel transforms on time series:

\begin{definition}[Hilbert space]\label{defn:hilbert_space}
    A vector space $H$ over a field $F$ with an inner product $\innerproduct{\cdot}{\cdot}_H: H \times H \rightarrow F$ that also defines a complete\footnote{A space $X$ is complete if every Cauchy sequence  converges in $X$. A Cauchy sequence is a sequence ${\{x_n\}}_{n \in \N}, x_n \in X$ with $\lim\limits_{(m,n)\rightarrow\infty} |x_m - x_n| = 0$.} metric space is called a Hilbert space.
\end{definition}

The key property of Hilbert spaces we wish to leverage is the norm induced by the inner product $\innerproduct{\cdot}{\cdot}_H$.
This inner product defines the higher order distance measure we wish to use on the raw observation space.

\begin{definition}[Kernel transform]\label{defn:kernel_trick}
    Let $X$ be an arbitrary space and $H$ be a Hilbert space with inner product $\innerproduct{\cdot}{\cdot}_H$.
    $\kappa(\cdot, \cdot): X \times X \rightarrow \R$ is a kernel transform if $\kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H$ for some $\phi: X \rightarrow H$.
\end{definition}

Note that in Definition~\ref{defn:kernel_trick}, the mapping function $\phi$ need not be explicitly defined.
In fact, $\kappa$ being a positive-semidefinite function (or matrix over discrete spaces) implies the existence of a satisfactory function $\phi$.
Thus, we can consider arbitrary higher-order distance measures as any positive-semidefinite mapping $\kappa: X \times X \rightarrow H$ guarantees a similarity measure in $H$.

\begin{definition}[Reproducing kernel Hilbert space (RKHS)]\label{defn:rkhs}
    Let $H$ be a Hilbert space of real-valued functions on an arbitrary set $X$.
    $H$ is a reproducing kernel Hilbert space if there exists a \textbf{reproducing kernel}, $\kappa_x$ $\forall x \in X$, where $f(x) = \innerproduct{f}{\kappa_x}_H~\forall~f \in H$.
\end{definition}

Note that $\kappa(x,y) = \innerproduct{\kappa_x}{\kappa_y}_H$, and thus the kernel transform in Definition~\ref{defn:kernel_trick} defines a RKHS.\@
This demonstrates that we can consider arbitrary higher-order similarity measures using kernel functions on the space of observed samples.
Furthermore, we can construct proper distance measures from the norm induced by the inner product on a RKHS.\@

\begin{definition}[Hilbert norm-induced distance]\label{defn:norm_distance}
    Given an input set $X$ and a reproducing kernel $\kappa$ for RKHS $H$: $\kappa(\cdot, \cdot) \colon X \times X \rightarrow \R$, define a distance measure on $X$ by:
    \begin{align*}
        {d(x,y)}^2 &= \norm{\phi(x) - \phi(y)}_H^2 = \innerproduct{\phi(x) - \phi(y)}{\phi(x) - \phi(y)}_H \\
        &= \innerproduct{z}{z}_H = \kappa(\phi^{-1}(z), \phi^{-1}(z)) \numberthis
    \end{align*}
\end{definition}

Note that we have still not explicitly defined the feature map $\phi(\cdot)$.
Instead, we contend that such a function exists that allows our transform to take place, which we prove in Section~\ref{subsec:proposed_kernel}.
In certain instances, we may wish to actually define an explicit mapping $\phi(\cdot)$.
In cases such as these, a distance measure induced by the kernel will arise much more simply.

\subsection{(Weak) Derivatives on Time Series}
\label{subsec:weak_derivatives}

Consider the following definitions of discrete time derivatives.

\begin{definition}[Discrete-time derivatives]
    Consider a discrete function $f \colon \Z^+ \rightarrow \R$ with samples $h$ apart.
    For $n \in \Z$, the following derivatives are defined.
    \hfill 
    \begin{enumerate}[(i)]
        \item \textbf{Forward difference:} $f'(n) = \frac{f(n+1) - f(n)}{h}$
        \item \textbf{Backward difference:} $f'(n) = \frac{f(n) - f(n-1)}{h}$
        \item \textbf{Central difference:} $f'(n) = \frac{f(n+1) - f(n-1)}{2h}$\\
            This is the average of the forward and backward differences.
    \end{enumerate}
\end{definition}

Note that these definitions imply continuous differentiability of a time series $t$ represented as a function $f(n)$.
We apply these definitions in Section~\ref{subsec:seminorms}, where it is necessary to consider derivatives of elements in a Hilbert space for application to distance measures.

\subsection{Seminorms in Hilbert Spaces}
\label{subsec:seminorms}

We have so far defined machinery which will, given a reproducing kernel $\kappa$ from the observation space, permit us to calculate higher order distances using the implicit feature map $\phi$ induced by the kernel function.
Recall the kernel transform:

\begin{align}
    \kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H
\end{align}

From this, the induced distance measure is:

\begin{align}
    d(x,y) = \norm{\phi(x) - \phi(y)}_H
\end{align}

However, we may wish to understand behavior even more explicitly in this space by considering a functions derivatives.
To this end, we consider the concept of \textit{seminorms}.

\begin{definition}[Seminorm]
    A seminorm $\norm{\cdot}_S$ on a space $S$ satisfies:
    \begin{enumerate}[(i)]
        \item $\norm{ax}_S = |a|\norm{x}_S$ for scalar $a$
        \item $\norm{x + y}_S \le \norm{x}_S + \norm{y}_S$ (triangle inequality)
    \end{enumerate}
\end{definition}

This has some benefits for time series classification which we seek to exploit.
\begin{itemize}
    \item \textbf{Shift-agnostic:} Let $f,g \in H$, a RKHS, and let $g = f + c$, where $c$ is a constant, and let both $f,g$ be $n$-differentiable.
        The norms $\norm{f}_H, \norm{g}_H$ will vary drastically depending on $c$.
        However, such shifting is not apparent with a derivative-based seminorm: $\norm{x}_S = {(\sum\limits_{k=1}^n \norm{D^k x}^p_H)}^{\frac{1}{p}}$.
        Thus $\norm{g}_S = {(\sum\limits_{k=1}^n \norm{D^k (f+c)}^p_H)}^{\frac{1}{p}} = {(\sum\limits_{k=1}^n \norm{D^k f}^p_H)}^{\frac{1}{p}} = \norm{f}_S$.
        Here, the operator $D$ is understood in a weak sense (see Section~\ref{subsec:weak_derivatives}).
    \item \textbf{Scaling behavior:} Consider the seminorm $\norm{x}_S = {\sum\limits_{|\alpha| = 1}^n(\norm{D^{\alpha} x}^p_H)}^{\frac{1}{p}}$.
        Let $f,g \in H$ be at least once differentiable with $g(x) = f(cx)$, $c$ a scalar.
        Then, $\norm{g(x)}_S = \norm{f(cx)}_S = {\sum\limits_{|\alpha| = 1}^n(\norm{D^{\alpha} f(cx)}^p_H)}^{\frac{1}{p}} = r^{p-n} \norm{f(x)}_S$ where $n$ is the dimension of $H$.
\end{itemize}

Expanding on Definition~\ref{defn:norm_distance} to create a distance metric based on a seminorm, we consider the following derivative-based distance measure:

\begin{align}
    d(x,y) = \norm{\phi(x)-\phi(y)}_S = \norm{D (\phi(x) - \phi(y))}_H = \norm{\phi'(x) - \phi'(y)}_H
\end{align}

If we are able to impose additional structure onto the feature map $\phi(\cdot)$, the seminorm definition has much greater value.

Suppose $\phi \colon X \rightarrow H$ is a feature map from input space $X$ to Hilbert space $H$, and that $\phi$ is surjective and once differentiable.
Then, $\exists z$ such that $\phi'(x) - \phi'(y) = \phi(z)$ since $H$ is a vector space closed under addition, and $\phi$ is surjective.

\begin{align}
    d(x,y) = \norm{\phi'(x) - \phi'(y)}_H = \norm{\phi(z)}_H = \sqrt{\innerproduct{\phi(z)}{\phi(z)}_H} = \sqrt{\kappa_H(z,z)}
\end{align}

where $\kappa_H$ is the reproducing kernel of $H$.

Note that while $d(\cdot,\cdot)$ as defined above is sub-additive~\cite{rosenlicht68-realanalysis}, it is not identically zero and is thus not a proper metric: $g = f + c$ meets $d(f,g) = 0$ even when $f \neq g$.

Given a ``nice'' feature mapping $\phi$ from input space to a Hilbert space induced by a reproducing kernel, we can more intuitively and effectively categorize time series behavior and time series distance using seminorms in these spaces.
However, this is not trivial, as Mercer's condition and theorem only guarantee the existence of $\phi$ given a positive semi-definite kernel and make no guarantees on its differentiability or surjectivity.
We explore the potential for seminorm usage in the next section with an explicit kernel for anomaly detection.

\section{Kernelized Hashing Model for Time Series}
\label{sec:kernel_hashing}

We now propose a specific hashing model for time series with the goal of anomaly detection.

\begin{definition}[Property $\mathscr{A}$]\label{defn:property_alpha}
    Let $\LSH$ be a $(2r, s, p, q)$-sensitive LSH family on $X$ with distance measure $d(\cdot, \cdot)$.
    $\LSH$ has property~$\mathscr{A}$ if the following holds:
    Fix $x \in X$ and construct $S = \{s \in X \mid d(x,s) \le r\}$.
    Then, $\Pr[h(x_i) = h(x_j)] \ge p~\forall x_i, x_j \in S, h \in \LSH$ 
\end{definition}

Property~$\mathscr{A}$ confers the notion that elements which lie within a ball of fixed radius should have high probability of hashing to the same value.
Furthermore, such families may be \textit{nested} with increasing values of $n$ to form neighborhoods with different levels of similarity.
We now define what we need from a distance measure $d$ to obtain this nesting property.

\begin{claim}\label{prop:property_alpha}
    An LSH family $\LSH$ with distance measure $d(\cdot, \cdot)$ on a set $X$ has Property~$\mathscr{A}$ if $d(\cdot, \cdot)$ is a sub-additive distance measure.
\end{claim}

\begin{proof}
    Let $\LSH$ be an LSH family on $X$ which is $(2r, s, p, q)$-sensitive and suppose $d: X \times X \rightarrow \R_+$ is sub-additive.

    Fix $x \in X$ and construct $S = \{s \in X \mid d(x,s) \le r\}$.
    Since $d$ is a sub-additive distance measure, $d(x_i, x_j) \le d(x_i,x) + d(x,x_j) = d(x,x_i) + d(x,x_j) \le 2r~\forall x_i,x_j \in S$.
    Thus by construction of $\LSH$, $\Pr[h(x_i) = h(x_j)] \ge p~\forall x_i, x_j \in S, h \in \LSH$, and thus $\LSH$ has property~$\mathscr{A}$.
\end{proof}

Property $\mathscr{A}$ additionally allows for a stratified LSH scheme using a set of LSH families index by a distance $r$ which allows us to confer a notion of closeness between buckets of a lower LSH strata.

\begin{definition}[Ranked LSH families]\label{defn:ranked_lsh}
    A set of hash families ${\{{\LSH}_r\}}_{r\in R}$ is a ranked LSH family $H_r$ has property $\mathscr{A}~\forall r \in R$.
    Denote such a set of families as $(R,p_1, p_2)$-sensitive LSH families ${\LSH}_R$.
    %, where $p = \inf\{ p_i | H_{r_1}$ is $(r_1,r_1,p_i,p_i)$-sensitive $\}$.
\end{definition}

% TODO: is this proposition necessary?
%In summary, we are now considering a \textit{ranked LSH family} with the following properties:
%\begin{proposition}\label{prop:ranked_lsh}
%    There exists $(R, p_1, p_2)$-sensitive ranked LSH families ${\LSH}_R = {\{{\LSH}_r\}}_{r\in R}$ such that the following hold:
%    \begin{enumerate}[(i)]
%        \item\label{itm:first}
%            $h \in {\LSH}_r \in {\LSH}_R$ collides with high probability if $y \in B_r(x)$: If $d(x,y) < r, \Pr[h(x) = h(y)] \ge p_1$.
%        \item\label{itm:second}
%            $h \in {\LSH}_r$ has few false collisions: If $d(x,y) > r$, $\Pr[h(x) = h(y)] < p_2$.
%        \item\label{itm:rank_property}
%            Property $\mathscr{A}$ holds for any two families ${\LSH}_{r_1}, {\LSH}_{r_2} \in {\LSH}_R$.
%    \end{enumerate}
%\end{proposition}

%\begin{proof}
%\end{proof}

This corroborates the notion of closeness to collision probabilities, which allows for the grouping of similar time series.
Due to Property~$\mathscr{A}$, we may apply iterative hashing scheme to \textit{rank} the probabilities of closeness based on the varied parameter $r$.
We discuss this in greater depth in Section~\ref{subsec:ranked_lsh}

\subsection{Kernel Model}
\label{subsec:kernel_model}

Consider a space of samples $S = \{x_1,\ldots,x_n\} \subset \R^d$ with similarity measure $\kappa \colon S \times S \rightarrow \R$ defined by $\kappa(x,y) = \innerproduct{\phi(x)}{\phi(y)}_H$, with $H$ being a RKHS.\@
We now consider the effects of a positive-semidefinite similarity measure $\kappa(\cdot, \cdot)$ as well as its corresponding distance measure $\tilde{\kappa}(x,y) = \|\phi(x) - \phi(y)\|_H$, the norm induced by the RKHS.\@
Note that from Definition~\ref{defn:kernel_trick}, we do not need to explicitly define the mapping $\phi(\cdot)$ to an RKHS.\@

\begin{definition}[Kernelized LSH]\label{defn:kernel_lsh}
    %Let $\kappa_r(\cdot, \cdot) = \kappa()$ 
    ${\LSH}_R = {\{ {\LSH}_r \}}_{r \in R}$ is a $(R, p, q)$-sensitive ranked, kernelized LSH family if, for any $x,y \in S$:
    \begin{enumerate}[(i)]
        \item If $\tilde{\kappa}(x,y) \le r, \Pr[h_r(x) = h_r(y)] > p$
        \item If $\tilde{\kappa}(x,y) \ge r, \Pr[h_r(x) = h_r(y)] < q$
        \item Property $\mathscr{A}$ holds for any LSH family ${\LSH}_{r_i} \in {\LSH}_R$
    \end{enumerate}
    Denote such a family a \textbf{RKLSH} family.
\end{definition}

We now consider how to apply a RKLSH family to time series for anomaly detection and construct such a family in Section~\ref{sec:proposed_scheme}.

\subsection{Ranked Hashing by Iteration}
\label{subsec:ranked_lsh}

Let ${\LSH}_R$ be a RKLSH family which approximates a measure on time series distance with $R$ being a set of stratifying distance thresholds, and let $S$ be the space of all possible time series.

% You can define a relation wrt a single point, for each point and for each threshold
% Triangulate based on which two fall into same class at each threshold
% Intuitively, probabilistic neighborhoods defined for < r_1 threshold
% < r_2 should confirm all of the < r_1 with high probability, forming a tier 2 neighborhood
% Two neighborhoods are ``ranked'' based on the lowest tier on which they reside in the same neighborhood

\begin{definition}[LSH neighborhood]\label{defn:neighborhood}
    A \textbf{neighborhood} with respect to distance threshold $r_i \in R$ and LSH hash function $h \in {\LSH}_R$ is a set of points $S$ such that $\forall s_i, s_j \in S, h(s_i) = h(s_j)$.
\end{definition}

The goal of hashing the set of input points is to efficiently compute sets of \textit{approximate nearest neighbors} (ANN) such that the behavior of each point can be classified.
However, the construction of RKLSH we have previously defined provides extra structure which allows us to make even stronger similarity claims:

\begin{definition}[Neighborhood rank]\label{defn:neighborhood_rank}
    Let $X$ be an input space and ${\LSH}_R$ an RKLSH, with $R = \{r_1, r_2, \ldots\}$.
    Two data samples $x,y \in X$ have \textbf{rank} $n$ if $n = \inf\limits_i\{ r_i \in R \mid \forall r_j \ge r_i,~\exists h_{r_j} \in {\LSH}_{r_j}$ such that $h_{r_j}(x) = h_{r_j}(y) \}$.
\end{definition}

We can thus apply the ranked hashing scheme to perform a stronger approximate nearest-neighbor calculation than with a single LSH family.
The similarity of any two points within a neighborhood can be ranked to give an indication of the confidence a point truly falls within a given neighborhood and thus exhibits the given behavior.

Thus, by applying this tiered ANN calculation, we are able to \textit{rank} the similarity of two elements by examining the threshold at which their hashes collide.

\section{Anomaly Detection on Time Series}
\label{subsec:anomaly_detection}
\label{sec:proposed_scheme}

The above theory provides intuition that similar time series may be grouped together and ranked by their closeness via a kernel distance measure $\tilde{\kappa}$~\cite{Hachiya13-NSH}.
In the context of anomaly detection, we can apply this scheme to a set of \textit{normal} traces (time series of utilization, label encoded syscalls, etc.) to determine what thresholds and clusters constitute normal execution with a finer granularity.

Instead of a binary label of \textit{normal} vs \textit{anomalous}, we stratify applications by behavior exhibited during execution.
Traces which do not match enough previously ``normal'' clusters over the set of thresholds may be considered to represent anomalous activity.

We now propose a specific distance measure and kernel transform to maximize the impact of our RKLSH algorithm for anomaly detection.

\subsection{Seminorm Induced Space}

Consider a Hilbert space $H$ over finite-length, discrete-time time series equipped with the following norm:

\begin{align}\label{eqn:norm}
    \norm{x}_H = \sqrt{\sum\limits_i{x_i^2} + \sum\limits_i{(D^1 x_i)}^2}
\end{align}
Where $D^1 s$ represents the time series of point-wise first derivatives of $s$.

This formulation arises from discussion of seminorms (Section~\ref{subsec:seminorms}) for their scaling and shifting behavior.
However, we show that this is in fact a proper norm and additionally explore the Hilbert space and inner product which induce this norm to derive an exact distance metric.

\begin{claim}
    The following seminorm is a proper norm:
    \begin{align*}
        \norm{x}_H = \sqrt{\sum\limits_i{x_i^2} + \sum\limits_i{(D^1 x_i)}^2}
    \end{align*}
\end{claim}

\begin{proof}
    \begin{align*}
        \norm{x + y}^2_H = \sum\limits_i{(x_i + y_i)}^2 + \sum\limits_i{(D^1 (x_i + y_i))}^2 \\
        = \sum\limits_i{x_i^2} + \sum\limits_i{(D^1 x_i)}^2 + \sum\limits_i{y_i^2} + \sum\limits_i{(D^1 y_i)}^2\\
        + 2 \sum\limits_i{(x_i y_i)} + \sum\limits_i{(D^1 x_i D^1 y_i)} \\
        \le \norm{x}^2_H + \norm{y}^2_H + 2 |\innerproduct{x}{y}_H|\\
        \le {(\norm{x}_H + \norm{y}_H)}^2 (\textnormal{by Cauchy-Schwartz})\numberthis
    \end{align*}

    \begin{align*}
        \norm{a x}_H = \sqrt{\sum\limits_i{(a x_i)}^2 + \sum\limits_i{(D^1 a x_i)}^2}\\
        = \sqrt{a^2 \sum\limits_i{x_i^2} + a^2 \sum\limits_i{(D^1 x_i)}^2}\\
        = |a| \norm{x}_H \numberthis
    \end{align*}

    \begin{align*}
        \norm{x} = 0 \Leftrightarrow \sqrt{\sum\limits_i{x_i^2} + \sum\limits_i{(D^1 x_i)}^2} = 0\Leftrightarrow x = \bm{0} \numberthis
    \end{align*}
\end{proof}

\begin{claim}
    The norm in Equation~\ref{eqn:norm} is naturally induced by an inner product of the following form:
    \begin{align}
        \innerproduct{x}{y}_H = \sum\limits_i(x_i y_i) + \sum\limits_i(D^1 x_i D^1 y_i)
    \end{align}
\end{claim}

\begin{proof}
    \begin{align*}
        \sqrt{\innerproduct{x}{x}_H} = \sqrt{\sum\limits_i{x_i^2} + \sum\limits_i{(D^1 x_i)}^2} = \norm{x}_H
    \end{align*}
\end{proof}

\subsection{Kernel Transform}
We now define a feature map from the raw observation space of time series $S$ to the constructed Hilbert space $H$.

\begin{claim}[Reproducing Kernel]
    Consider the feature map $\phi(\cdot)$ from $S$ to $H$ with norm as defined above:
    \begin{align*}
        \phi \colon S & \rightarrow H\\
        s &\mapsto (s,D^1 s) \label{eqn:phi}
    \end{align*}
    $\phi(\cdot)$ defines a reproducing kernel $\kappa \colon X \times X \rightarrow H$ with $\kappa(f, g) = \innerproduct{\phi(f)}{\phi{(g)}}_H$ for $f,g \in X$ and $\norm{h}_H = \sqrt{\sum\limits_i{h_i^2} + \sum\limits_i{(D^1 h_i)}^2}$ for $h \in H$.
\end{claim}

\begin{proof}
    We first note that $\phi(\cdot)$ is a linear map:
    \begin{align*}
        \phi(a x + b y) = (a x + b y, D^1 (a x + b y)) = a (x, D^1 x) + b (y, D^1 y) = a \phi(x) + b \phi(y)
    \end{align*}

    $\kappa(f, g) = \innerproduct{\phi(f)}{\phi(g)}_H$ is an inner product on $H$, and therefore defines a unique positive-definite kernel (by the positive-definite property of the inner product and linearity of $\phi(\cdot)$).
\end{proof}

\subsection{Distance Approximations in Real-Time}
\label{subsec:time_series}

\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \SetKwData{Raise}{Raise}
    \Input{$t = (\ldots, t_i, \ldots) \in S$, real-time time series;\\
        $R$, set of thresholds;\\
        $K$, window sizes;\\
        $\phi \colon S \rightarrow H$, feature map;\\
        $b$, number of bands
        $a$, number of hash functions per band;\\
        $M \in \N$;\\
        $d \colon \R^{\dim(H)} \times \R^{\dim(H)} \rightarrow \R^+$, distance metric;\\
        $A(r,h)$, function on hash values for each $r \in R$. $A(r,\cdot)$ maps known inputs to $1$, unknown (anomalous) inputs to $0$.
    }
    \Output{Behavior hash value for time series at every $\floor{\frac{k}{2}}$ points.\\
        Signals anomalies when detected}
    \If{$\mathtt{len}(t) \bmod K[0] = 0$}{
        \For{$k \in K$}{
            Let $t_k$ = $t[-k:]$\\
            $G = \{ {[h_1, \ldots, h_a ]}_1, \ldots, {[ h_1, \ldots, h_a]}_b \}$ \tcc*[f]{ construct $b$ sets of hash functions, each with $a$ hashes }\\
            $\mathcal{P} = \{ \{ h_i(\phi(t_k)) \mid h_i \in g_b \} \mid g_b \in G \}$ \tcc*[f]{hash $\phi(t_k)$ (project onto $a$ random hyperplanes per band)}\\
            Define $v_r(\mathcal{P}) = \{ \min\limits_{p \in P}[\frac{d(\bm{0}, p)}{r} \bmod M] \mid P \in \mathcal{P} \}$\\
            $D = \{ v_r(\mathcal{P}) \mid r \in R \}$ \tcc*[f]{min distances from origin to projected point for each unit $r$ for each band}\\
            Define $R_i = \{ r_j \in R \mid j \ge i \}$\\
            \If{$\exists~R_i$ such that $|\{A(r,v_r_j(P)) = 0 \mid \forall r_j \in R_i\}| \ge \frac{\cardinality{R_i}}{2}$}{
                \Raise{anomaly}
            }
        }
    }
    \caption{Real-time Behavioral Hash for Time Series}
\end{algorithm}

\begin{claim}
    The above algorithm defines a $()$-sensitive Hash family.
\end{claim}

We have so far defined a kernelized hash family (Definition~\ref{defn:kernel_lsh}) over some arbitrary space $S$, which assumes that we treat an entire (finite) time series as a single element in this space and compute distances on these full time series.
However, to compute distances between time series in real-time, we must make some adjustments to account for each time series being an incoming stream of data.

To determine the perceived behavior of time series in real-time with respect to a distance measure $\tilde{\kappa}(\cdot,\cdot)$, we \textit{segment} the time series into overlapping windows of length $k$, with an overlap of $\floor[\big]{\frac{k}{2}}$ points.
We now construct a RKLSH family over the space of such $k$-length time series segments to satisfy the above theory.

In the context of real-time anomaly detection, this has the advantage of decomposing examined time series into finite time series of the same length, reducing computation by restricting time series length to a variable parameter, and allowing us to define \textit{behavior over time} as we hash each segment independently.



\iffalse{}
\section{Results and Evaluation}
\subsection{Dynamic Time Warping}

% Empirical evaluation of DTW on traces *for anomaly detection*
\todo{We speculate that DTW has a fundamental weakness when dealing with time series.
    ``Stretching'' and ``folding'' time series to fit a distance measure errs on the side of similarity, which is unacceptable for anomaly detection.}

\subsection{Our Scheme}

\todo{Proof of hash approximating kernel distance measure, evaluation of detection probability, and algorithmic complexity evaluation.}

\fi

\bibliographystyle{unsrt}
\bibliography{../bibliography/bibliography}

\end{document}
